# Akash Network - Support Special Interest Group (SIG) - Meeting #41

## Agenda
- Opening remarks and overview  
- Triage of awaiting issues in the support repository  
- Review of open community and engineering issues  
- Discussions on insider support, moderation, and deployment tools  
- Wrap-up and action items

## Meeting Details

- Date: February 19, 2025 
- Time: 07:00 GMT-7
- [Recording](https://slfttinehkclmi42ykddxhbyfhy5dqyzzwpckpqzxppedagrbpsq.arweave.net/kss5oaQ6hLYjmsKGO5w4KfHRwxnNniU-GbveQYDRC-U)
- [Transcript](#transcript)

## Participants
- Tyler Wright  
- Scott Carruthers  
- Andrey Arapov  
- Zeke Ezagui  
- Rodri R  
- Pavlo Dereniuk  
- Deathless  
- B S  
- Damir Simpovic  
- Dimokus  


## Meeting Notes

### Opening remarks
- Tyler Wright opened the SIG Support monthly meeting and outlined its purpose: reviewing GitHub issues related to the Akash network, triaging unresolved items, and discussing common support themes across Discord and Telegram.
- Participants were encouraged to raise new issues or drop items in the chat for inclusion in the agenda.

### Triage of awaiting issues in the support repository
- Scott Carruthers led the triage session:

  - Network upgrade for long queries  
    - Issue was raised due to increasing blockchain size causing slow query performance for providers.
    - A network-wide upgrade is scheduled to improve indexing and query performance, likely to be deployed the following week.
    - The issue was reassigned to the node repository.
    - Rodri R clarified this was unrelated to the Cosmos SDK 47 upgrade. Scott confirmed it's a separate, urgent upgrade with no hardware changes.

  - Subdomain field abuse risk  
    - A core team member raised a concern where someone could deploy with a subdomain they do not own (e.g., `scott.com`) and prevent the actual owner from deploying on the same provider.
    - This does not affect all providers, only the one where the initial deployment was made.
    - Manual intervention can resolve this, but an automated validation system was proposed for future consideration.
    - Issue moved to the provider repo and assigned to Scott and Artur.

  - Pod spawn and eviction from low ephemeral storage  
    - Andrey Arapov explained a case where pods with minimal storage repeatedly downloaded large models, leading to constant restarts and evictions.
    - This behavior leads to a buildup of failed pods, degrading performance.
    - A workaround was implemented to clean up failed pods every 10 minutes.
    - A new provider version (0.6.7) may resolve the issue by limiting pod revision history.
    - Andrey noted the issue is rare and low-priority, but worth testing in future provider versions.

### Additional issue discussed: GPU label length
- Zeke Ezagui raised [Issue 233](https://github.com/akash-network/support/issues/233) involving a Kubernetes limitation: GPU labels exceeding 63 characters (e.g., RTX 4080 SUPER) cannot be applied, blocking usage of certain GPUs on the network.
- Andrey Arapov confirmed awareness of the issue and suggested using shorter node names to avoid label collisions.
- Tyler Wright committed to following up and ensuring the issue receives short- and long-term attention.

### Insider and community support topics
- Tyler Wright gave an overview of the Akash Insiders, Vanguards, and Navigators programs, which offer community support via Discord and the console.
- Rodri R asked about a recent console outage that coincided with a CoinGecko issue.
  - Tyler clarified the incident was related to Keplr, which affected CoinGecko price feeds and, in turn, the Akash console.
  - Vanguards were credited for swiftly identifying and helping resolve the issue.

- Deathless asked about recommending new provider installation methods instead of Praetor.
  - Tyler explained the preferred method is using the CLI build process documented on the Akash website.
  - For users less comfortable with CLI, the Akash Provider Console is currently undergoing general release testing.
  - Praetor will be deprecated in the future, but is still an option for those unable to use CLI during the transition period.

### Announcements and closing notes
- Tyler reminded attendees about the SIG Analytics meeting taking place the next day, which will focus on indexer support and community discussions.
- Mentioned an upcoming ETHDenver event working group meeting later in the day.
- Thanked participants for their time and acknowledged the communityâ€™s continued support and feedback.

## Action items
- Scott Carruthers:
  - Finalize triage on long-query issue and coordinate the upcoming network upgrade.
  - Test pod eviction handling in version 0.6.7 and follow up internally.
  - Investigate subdomain deployment misuse and propose long-term validation improvements.

- Tyler Wright:
  - Follow up on Issue 233 (GPU label length) and ensure continued progress.
  - Support messaging around the CoinGecko/Keplr incident and clarify its impact on the console.
  - Promote best practices for provider setup and coordinate deprecation messaging for Praetor.

- Andrey Arapov:
  - Assist in testing the pod eviction fix in future provider versions.
  - Propose documentation updates around using short node names.

- Community team:
  - Monitor and assist users with provider setup via CLI or the new Akash Provider Console.
  - Escalate unresolved Discord support queries and coordinate with core team when necessary.


## **Transcript**

Tyler Wright: All right, welcome everybody to SIG support monthly meeting. It's February 19th, 2025. During the SIG support monthly meeting, the group goes over issues that are open inside the clash network organization in GitHub.  The support repo is where all issues related to the core code live. There's also another repo for console where again all aos console related issues live. usually during these meetings what we do is we see if there's any issues that are awaiting triage and we triage them led by members of the core engineering team.

Tyler Wright: Following that, we see if there's any other issues that are open in the, support repo that anybody else wants to discuss in more detail. after we go through the issues in the support repo, the group, usually there's a number of folks from the insiders or vanguards on the call. they talk about just general support inside Discord, Telegram, and other, channels. If anybody has anything else that they want to specifically talk about, feel free to drop a note inside of the chat and we can go and just make sure it gets discussed and added to the agenda. Otherwise, I'll hand it over to Scott who usually leads us in triaging any issues that are awaiting triage.

Tyler Wright: So, Scott, feel free to take it away.

Scott Carruthers: All right.

Scott Carruthers: Thanks, Hillary. let me share my screen. So, I think, covering the issues that are currently not assigned and going through this triage process should be rather quick this morning. So, I'm going to go over a couple and then Andy, if you want to speak on one that you opened. Actually, it looks like almost a month ago. I actually haven't reviewed this before the call.  So maybe you could give some insight. looks like there's an update as well. So I'll go through the first couple and then Andy possibly you have some thoughts on the third. so beginning again I think these are relatively easy. Tyer, you opened case. So it looks like I'm actually not sure why this is in a waiting triage. It's already been so I will take it out of assigned and then I'll add the label.

Scott Carruthers: This is a blockchain upgrade. So I'll put it in the node repository. so I think that is it if anyone's not aware of the nature of this issue. So we're based on the size of the blockchain and recent experiences with how long it takes some queries to execute. we're doing some react reindexing and are going to be doing a urgent chain upgrade sometime later this week or I would guess at this point it's probably going to be sometime next week.

Scott Carruthers: So these long queries based on blockchain size and number of orders that have been created historically on the AOS network. This affects some provider queries when the pop first starts and looking for existing orders on the network and other downstream effects as well. But this is definitely affecting providers. so we need to make these queries and indexes more appropriate for this current blockchain size. so again, this is a network upgrade that we'll be sharing more details on and I think it will probably most likely happen next week. Any questions on the network upgrade? so going back.

Scott Carruthers: That should just leave us with two awaiting triage. this is a issue that was open by a core team member on our front end development team one of our primary console developers. I don't think this is an urgent issue at all. but he opened an issue for future guard rails against the ability for someone to create a deployment and in that deployment include a sub field and there be no proof of ownership of that domain.  So an example, so if I had scott.com is a domain that I own, someone else on the Akash network could create a deployment using the sub field of scott.com on the Europlots provider.


### 00:05:00

Scott Carruthers: there's checks within the blockchain and node repository to ensure that a set field in a specific domain can only be used on a single deployment.  So, if I'm the true owner of scott.com and then someone fictitiously or for some reason had some cause to want to deadlock or black hole scout.com, they could create a deployment and put scout.com in the SSF field and as a true owner would then not be able to create a similar deployment.

Scott Carruthers: This would only affect a deployment on the same provider that as the true owner, I could definitely create a deployment using scott.com on any other provider, but I wouldn't be able to create it on that Europlas provider. So again, I don't think this is really an urgent issue. certainly based on some of the other things that we're dealing with and have focus on. but in the future, just to guard against this, we would, create some kind of a sanity check with an integration that would check the ownership of a domain. and that would go through that validation before you could actually use it in the interm proved that this was going on they could contact the europos provider and ask them to kill the malicious or the deployment that lacks ownership of that domain. So we could certainly deal with this manually on the provider. but this is a suggestion from one of our core team members to make an automated process and have some more intelligent guard rails in the future.

Scott Carruthers: So I will take this out of awaiting triage, and put it in the provider repo. I'll just assign this both to, myself and Archer for now. Yeah, go ahead, Rder. No.

Rodri R: Hey Katari, just going back to the urgent network upgrade that you mentioned before, this isn't the same as the one we've been beta testing and are we going to need to upgrade our hardware as specified before?

Scott Carruthers: So I think you're referring to the test net that we've conducted for the Cosmos SDK 47 so this is not the same as that hardware upgrade should be necessary for pre-existing validators or RPC nodes. this is a very minor change with some index improvements and streamline behavior. So yeah, this is completely different than the Cosmos SDK 47 upgrade that will probably be happening several weeks after. this is an emergency upgrade just to streamline some indexing inquiries that are affecting the K providers.

Scott Carruthers: Does that answer the question?

Rodri R: Yeah, perfect. Thanks. That'sâ€¦

Scott Carruthers: Okay.

Rodri R: what I thought.

Scott Carruthers: Yeah.

Rodri R: Just wanted to confirm.

Scott Carruthers: No problem. any questions or concerns about this current issue and the lack of validation for domain ownership in Aash deployments? and going back and like I said, Andy, possibly if you would, possibly be some of some assistance. I didn't have a time to actually go through this issue,â€¦

Andrey Arapov: Yeah, sure.

Scott Carruthers: would you be able to just give us on the nature of the issue?

Andrey Arapov: Yeah, so the nature is basically when you have a deployment that requires less or very small amount of ephemeral storage and this deployment download something like some large model let's say I don't know 10 gig from hugging face. So what happens is that this pod fails to start it gets evicted and if you scroll slightly down the issue you will be able to see you see that there's a bunch of pods in error state.

Andrey Arapov: So that what happens so Kubernetes tries to evict the pot because it breach the required disk space because it keeps starting again keeps downloading keeps crashing because of not enough ephemeral space given to the deployment. what happens is that you get lots of ports in error state and it affects the operator and entry.  we can see the slightly above there is a large number of CPUs reported and basically it affects the providers in this way. and if you scroll down to the bottom I already added a comment also Vignish added a comment I think in January a month ago exactly months ago.


### 00:10:00

Andrey Arapov: so Vignish implemented a workar around I suggested basically it deletes the ports running in a failed state every 10 minutes. So to avoid this kind of situation where it goes this lots of ports running out of space it's just a workar around because we cannot really know who deployed their deployment based just on deployment sequence and a cash account address. because we do not know it and we cannot know it and we do not operate all providers just very few because of that it's the only way we could tackle it by this work around for now and there is a newer version of provider was released last week by Arthur 067 and I added comment four minutes ago. Yeah.

Andrey Arapov: So it's possible fix this issue only possible because it limits the pod resource created to only one revision history. So it shouldn't be able to spawn more than a single replica possibly but needs to be tested.

Andrey Arapov: And this kind of thing that I've seen it's quite rare. I just seen it couple of times throughout the past four years. Yeah. usually people they fix their deployment if it keeps crashing and restarting or not running. So it's quite rare to see. Yeah.

Scott Carruthers: And yeah,â€¦

Scott Carruthers: as you were covering that, I did recall the report of this. it's been more than a month, but yeah, I do recall the details of this And we should definitely test this in 067 or now was probably going to be 068 with some additional bug fixes. so yeah.

Scott Carruthers: Okay, sounds good. That all makes sense.

Andrey Arapov: Yeah. Yeah.

Andrey Arapov: And in terms of it's still awaiting trash, I wouldn't call it like it's not a P1, it's not even P2. because, it's possible to tackle it. so I would call it kind of, â€¦

Andrey Arapov: 2.5 or I mean two and a half or three in terms of priority. Yeah. And it's again like I didn't see it in wild too much. Usually people who deploy something it's not in their best interest to keep their application crashing. they normally would notice it and add more space so it doesn't crash.

Scott Carruthers: Yeah. Yeah.

Scott Carruthers: All right. Yeah. Thanks for that detail. and like I said, I actually remembered the details of this as you started to review it. So, it all sounds good. so, yeah, if just for a sanity check, if first of all, any questions or thoughts on the last issue that we just covered. and I think with that, I'll just do a sanity check, but I think if we go back to a waiting triage, that takes care of everything. pretty quick and easy this month. so if there's no other questions or concerns, I'll hand it back to you, Tower.

Tyler Wright: Excellent. Go ahead and seek

Zeke Ezagui: So in the chat I brought up an issue. It's 233. basically it's this kind of multiaceted issue which comes down to a Kubernetes limitation to where labels can't be more than 63 characters long. and what that does is, the most recent iteration of it is it blocks certain GPUs from being able to be on network right now. For example, the 4080 super is too long. So, we're not able to actually label that using inventory operator. And so, I just wanted to bring it up.

Zeke Ezagui: there's a couple other GPUs that can be affected. just wanted to bring it up and get y'all's thoughts and see what we think the best way moving forward is.

Andrey Arapov: Yeah, I just want to add that we aware of this issue and basically the idea is to keep the Kubernetes node name short because in this particular issue if you could check it further the original author of this issue he had the node I think 20 25 characters long and usually we normally use four five characters long five node one node two node And we don't see this show this kind of configuration, this kind of setups. And I think I'm not sure if we update documentation on that. If not, we definitely should. otherwise, yeah, we are aware of this issue.


### 00:15:00

Tyler Wright: All right. I know that there's been some comments that have been dropped in by some folks as recently as three weeks ago. it may be good for a member of the core team. I know Zeke has dropped in some thoughts there, but member of it might be great for another member of the core team just to drop in some additional notes and see if there's anything that we can do. I think this has been brought up a couple times as of recently. so again, maybe we could take some time after this call. I can have this action item to make sure that this is followed up on and we have some sort of short-term and long-term resolution. Any other thoughts on issue 233? All right.

Tyler Wright: any other issues that are open inside the support repo that anybody wants to discuss in more detail or ask questions on or give feedback. All Thank you, all right.  Next up, as a part of the agenda, we usually look to hear from members of the insiders vanguards to see if there's anything worth talking about with this group in terms of support.  Again, just a quick refresher for those that don't know, the Kasha Insiders program is an ambassador program run through the community for those it's completely voluntary for those that are interested in again being involved ambassadors and providing you support and information to folks inside Discord.

Tyler Wright: inside the insiders program, there's a number of other programs where folks can earn rewards, including the Vanguard's program, which is designed to give 24 by7 support inside Discord channels. So, if anybody's coming into the Discord and looking for, how to get involved, technical support, etc. there's a group of vanguards, that are available 247 to provide highle answers.  There's also a group called the Akash Navigators which is designed to give more solutions engineer I know as of recently there's ways for individuals from the community to get support from Akos Navigators via the AOSH console and via the website. there's a form that people can fill out and then again via the AOS insiders program will follow up and provide some solutions engineering support if it makes sense.

Tyler Wright: There's a number of other programs that are built out of the Akash insiders. So again, if you're interested in getting more involved with the Akash network and the Akash community, please go to the website and there's an application process for being an insider.  I just want to see if there's any insider vanguards or anybody else on the call that might have some themes that they want to discuss it pertains to, maybe some issues that they've seen happen on a number of cases inside Discord or elsewhere. So again, does anyone have any support related items or themes that they would like to discuss with the group here today?

Tyler Wright: I'll take that as a no. I know that historically we've talked about bots and bot activity inside of Discord. I know that there has been some measures put in place as of recently to continue to combat that. I know that again during the Akash insiders calls the group gets together and talks about some of these ways to combat some of this activity and protect the community inside Discord Telegram and elsewhere.

Tyler Wright: So appreciate all those that are getting involved giving feedback u making sure that again members of the Akash community inside discord aren't falling victim to scam activity that may be happening in DMs or elsewhere. So much appreciate the support from the insiders vanguards and just the general AOS community to help each other out and make sure everyone stays safe.  Go ahead, Roger.

Rodri R: Hey, so just quick question. Probably someone in here knows it. I think it was the day before yesterday that Coin Gecko seem to have an issue with AKT and with other tokens and seemed that at the same time the console went down. my guess is that prices from USD and AKT and USD get from pull from coin gecko and that's why they weren't working or the console wasn't working properly and there were a few people asking about it in Discord.


### 00:20:00

Tyler Wright: Yeah, I'll make sure that this gets followed up on, but as I understand it, this was a Kepler related issue that then had an effect on console because console uses Coin Gecko data to do some stuff with USD and AKT for part of the user experience workflows.

Tyler Wright: but again I believe this was not a console related issue but a Kepler related issue and again major shout out to members of the Akash Vanguards who were on this issue in the night I think this happened around 3:00 in the morning Eastern time so a number of members of the core team were asleep at that time working on other things and a couple of members of this vanguards found the source  of the issue, contacted support for Coin Gecko, Kepler, and others, and we're able to rectify this issue rather quickly. So again, I'll make sure that if anybody has any additional questions in Discord, I'll make sure those get Roger, if you can tag me in insiders to some of those conversations, that'd be great.

Tyler Wright: But yeah again as I understand it this was a Kepler related issue that then had an impact on Coin Gecko because of the Kepler related issue and again because console leverages coin gecko for a part of some calculations it had an effect on direct issue with console, just an issue as it pertains to some of the other workflows that affect the console. But thank you for calling that out, Roger. And again, I'll make sure that any messaging is updated.

Tyler Wright: I know Deflus has put a question in the chat around should we be referring people to the new way to install providers instead of using Prader. I think there was a SIG clients call yesterday as I understand it. There's a couple of things that are happening at the same time.  The prov Akos provider console is again in the process of general release right now. There are a number of community providers and data centers that are currently testing the new AOS provider console experience. that testing is closely being monitored by members of the core team on the product side.

Tyler Wright: and again feedback will be made soon. I think in terms of referring people to the new way to install providers there is some thought that for those that have the ability sharing the CLI documentation that is available at aost network doccks is always the number one way that the core team suggests on people building providers.

Tyler Wright: I know the core team has just started to think about leveraging Anible again. I know that again there's using K8s right now. there are some K3s at play for building some providers. But I think ultimately the core engineering team and DevOps group is looking at solutions around Anible because it's easier to change things as you're building and upgrading the provider.  So again this is like a project the core engineering team is actively working on.

Tyler Wright: I would say the best path is if somebody is technical enough we would like to refer them to the documentation a kasha network doc for building a provider I think that is using k8s if that's beyond their skill set and it's something they can't figure out then I think we're going to push them towards prator until aos provider console is in a state where we feel extremely comfortable and then again praor is going  to have an end of life that will be shared with members of the community. So again, there's continued kind of like a general release testing. I know there's members of the community.


### 00:25:00

Tyler Wright: yeah, so again to that announcement, Deathless the provider console is certainly a way for folks to be able to build provider and there's certainly a large amount of people that may be slightly less technical that we do definitely want to p push in that direction. but I would say the first referral should always be the documentation and see if folks can build be v be v be via the provider build scripts on the documentation. Any other thoughts, questions, concerns about what I just said? Thank you very much for that question.

Tyler Wright: deathless I would say deficit and if somebody does not want to use via the Kosh documentation and they want to build via GUI then again we'll use a Kosh provider console and then we'll just make sure that we can more closely monitor that group and point them in the direction of the channels  just in case they have any issues like the Kos provider channel or excuse me a kosh console channel because again we feel confident with where the product is and what it could do but I know there's some still ongoing testing and again the preferred way is always using the provider build scripts via the documentation because it's a lot easier to change things and customize the provider

Tyler Wright: Any other questions, comments, concerns about anything support related? If not, then I do want to remind folks, especially technical folks, there is going to be a SIG Analytics call tomorrow. Again, the SIG Analytics group doesn't meet all that often.  I think they meet once a quarter, but there is a team that has been talking to members of the core team about supporting the indexer and adding additional features. They put up a discussion and they're going to be talking about their discussion at the upcoming meeting tomorrow for SIG Analytics. So again, if anybody wants to get involved in that discussion, ask specific questions, I'll drop the discussion in chat.

Tyler Wright: make your voice heard in discussion or feel free to join the SIG Analytics meeting tomorrow. All right. If nothing else, and I know that some folks will be at ETH Denver next week if you're going out there. Safe travels. again, there's a working group for the events to talk about ETH Denver and some of the other upcoming events happening later today. again, much appreciate Scott, Andre, Zeke, and others going through, active issues, triaging any issues, and discussing some of the other issues. I'll make sure that there's a follow-up plan for issue number 233. and again, look out for more updates about the intermediate network upgrade that will be happening prior to the Cosmos SDK 47 migration. So, looking to have that onchain sometime soon.

Tyler Wright: I know it's in final testing right now. and then again we'll have a plan as it pertains to the network upgrade schedule. If there's nothing else then appreciate everyone's time today. again we can continue to stay connected on Slack and on Discord. again, if anybody has any other questions, comments, concerns, feel free to drop them in six support Discord channel. but hope everyone has a great rest of their week. Thank you to all those Vanguards, insiders, etc. that provide countless support inside Discord and other channels.

Tyler Wright: Much appreciate you all and thank you for all the testing that you do as we get ready for again u console improvements Cosmos SDK 47 migration intermediate upgrades again big shout out to all members of the community that do a great deal of testing of provider experiences deployment everything so y'all appreciate everybody that's listening to this recording later I'll make sure the notes and recording are available soon but I hope everyone has a great rest of the  and we shall talk soon. Bye-bye.

Scott Carruthers: Thanks

Zeke Ezagui: Thanks everybody.

Andrey Arapov: That's all.


### Meeting ended after 00:35:31 ðŸ‘‹


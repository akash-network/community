# Akash Network - Providers Special Interest Group (SIG) - Meeting #32

## Agenda

* Updates on provider notifications
* Akash at Home testing & architecture deep dive
* Upcoming provider software release and bug blocking
* Community testing and upcoming upgrade

## Meeting Details
- Date: Wednesday, August 27, 2025
- Time: 08:00 AM PT (Pacific Time)
- Recording: Coming Soon
- [Transcript](#transcript)


## Participants

* Zeke Ezagui
* Jigar Patel
* Deval Patel
* Damir Simpovic
* Scott Carruthers
* Benjamin B
* Anil Murty
* Vignesh V
* Zeke Ezagui
* andrew
* oiclid

## Meeting Notes

### Provider Console Update (Jigar Patel)

* Active development on the new *provider notifications* feature:

  * Allows providers to notify deployers of upcoming maintenance or unplanned downtime.
  * Notifications will surface in the **deploy console**, where deployers can subscribe to updates from specific providers.
  * This lays the foundation for more notification types in future updates.
* Automated notifications are in the pipeline:

  * Examples include hardware changes (e.g. GPU or drive updates), provider downtime, or network disruptions.
  * Notifications will be triggered even if the provider cannot manually send updates.
* These features will help providers proactively communicate availability and reliability status to deployers.

---

### Akash at Home: Architecture & Test Insights (Damir Simpovic)

* **Akash at Home** is a redesigned provider model optimized for remote, decentralized nodes (e.g. home setups).
* Key networking components:

  * Uses **TailScale** or **Headscale** (self-hosted version) as the secure overlay network.
  * Control plane node (ingress) is public-facing; all others can be behind NAT.
  * Supports **IPv6** for efficient routing; allows direct node-to-node communication.
* **Scalability findings:**

  * System remains stable up to 150–250 nodes depending on network churn.
  * Performance degrades beyond 400+ nodes, especially due to **headscale** CPU limits and topology recalculation overhead.
* **Routing and connectivity findings:**

  * Calico CNI with IBGP routing works well when combined with Headscale for advertisement.
  * VXLAN and IP-in-IP encapsulations introduced broadcast storms or routing loops, and are not suitable.
  * Central etcd cluster for Kubernetes control plane avoids issues when nodes drop from TailScale network.
* **Bandwidth impact is minimal** as most connections are direct; the headscale server primarily handles routing advertisements.

---

### Provider Software Release Status (Scott Carruthers)

* A new **provider release** is planned prior to the September network upgrade.
* Zeke has already built provider images and deployed them in testnet clusters.
* However, a **critical bug** is blocking the release:

  * After a few hours, provider pods lose communication with their RPC nodes.
  * Restarting pods temporarily resolves the issue, but a long-term fix is in progress.
* No firm ETA yet, but community should **stay alert via Discord** for testing and release updates.
* This release is expected to include **JWT support**, which is mandatory before the upcoming network upgrade.

---

## Action Items

* Finalize and release the provider notification system in both provider and deploy consoles.
* Continue scaling and refining **Akash at Home** implementation, addressing HeadScale performance and routing limits.
* Resolve the RPC disconnection bug in the provider version under test.
* Release provider software update with JWT ahead of the September network upgrade.
* Maintain clear communication via Discord for all release updates and testing opportunities.

## Transcript


### 00:00:00

  \
**Zeke Ezagui:** Welcome everybody to Sig Providers. Today is Wednesday, August 27th, 2025 and I am not Tyler if you can't tell. So, I'm stepping in for Tyler today. Um, but it should be same business as always. Um, so to start out this meeting, we'll have um Jigger or Deval give updates on provider console. Take it away. \
**Jigar Patel:** Thanks. So, um we've been working on uh couple of new features uh for provider console uh mainly for provider notifications. So, uh this feature will enable providers to uh give out notifications about any uh downtime or any maintenance uh uh they're doing to the provider um in upcoming days or week. Um so they can notify their uh deployers uh about that any um uh unplanned notific unplanned downtime uh goes that that notification goes to uh deployers who deploy onto that um uh provider. So yes uh it's pretty big uh feature because uh we will enable this into deploy console as well. So uh all the deployers will have ability to subscribe to that provider uh notification particular provider notification. \
  \
 


### 00:01:30

  \
**Jigar Patel:** Uh and uh once we have this structure we'll have more uh notification types. Um currently we are starting with uh uh any maintenance um um notifications and then uh we'll have some  automated uh notification as well uh in upcoming feature uh where u any let's say if you have any GPU changes or if you have any um drive changes uh and you want to notify that uh to your deployers uh you can also do that. uh some automated uh uh notifications as well where uh your network goes down and you are not able to notify uh your deployer but uh our system will auto automatically notifies uh any downtime to unpl downtime to their deployers as well. So yeah uh pretty excited about that. We are also working on uh Akash at home with Damir and Deal and Anil. We are all together. We are uh uh right now doing the P testing um where uh they can uh chime in to this. But uh we have a pretty good uh stable  cluster at this point of time with uh more than 400 500 nodes. \
  \
 


### 00:02:58

  \
**Jigar Patel:** Um and we are adding more to that. Um we're actually like stress testing it right now uh to find out any limitations with the um uh with the tails with the wire guard network uh or any uh internal components that we are using. But yeah, that's a update from my side. Uh thanks everyone. Go ahead. \
**Damir Simpovic:** I can give you a short uh presentation about the Akash at home and what we achieved so far if you want. \
**Jigar Patel:** Sure. \
**Damir Simpovic:** Sec. \
**Zeke Ezagui:** What did you say, Deir? \
**Deval Patel:** You're mute if you are speaking. \
**Damir Simpovic:** No, no, I know. Uh, just give me Can you hear me? \
**Zeke Ezagui:** Yes. \
**Deval Patel:** I can hear you \
**Damir Simpovic:** Okay. Just uh I'm preparing the \
**Zeke Ezagui:** Okay. While while Damir prepares that, does anybody here have any questions for the provider console team? Perfect. \
**Damir Simpovic:** Okay. Um, so a cash at home, can you see the the presentation, guys? \
**Zeke Ezagui:** Yes, we can. \
**Damir Simpovic:** Okay. All right. \
  \
 


### 00:05:02

  \
**Damir Simpovic:** So what is Akash  home? It's a new provider design. It's using tail scale and or headscale as the underlying network. So basically what that means is that the worker nodes uh do not have to be local, they can be remote. Uh they can even be behind that. Um they will still uh everything will still work correctly. uh the only node that does not need to be behind that is the ingress and where the uh provider uh uh will be exposed. So basically the nodes that will be um uh set up in the public DNS to uh where basically the contact point with the world will be. Um it works with uh IPv6. So basically uh a lot of uh new modern um internet services uh actually uh come with uh IPv6 uh like the whole 48 network routed to the home and the uh internal devices actually do get the public u IPv6 addresses. If that is the case uh then uh the tail scale or head scale in this case traffic will be encapsulated within IPv6 and uh the direct connection between nodes will be established over IPv6. \
  \
 


### 00:06:46

  \
**Damir Simpovic:** Um it's super efficient because uh del scale and headscale basically looks for a way to connect directly if possible. Um there is a slight overhead uh of around uh uh 60 bytes. Uh but um uh our our testing uh proved that uh a bit more conservative uh maximum transfer unit or MTU has to be used simply because uh some of the uh some of the internet service providers or mobile networks for instance um they have a lower MTU and uh yeah um and it's uh pretty scalable. We tested at over a thousand nodes. Uh it did start falling apart at that scale. Uh especially the headscale server. Uh so yeah, the sweet spot that uh we saw that uh everything was working nice and uh crisp and fast was around 250 nodes, maybe 150 nodes. that depends on how much changes uh like live changes to the network is are being uh you know if if there's constantly nodes disconnecting reconnecting and stuff then the headscale server basically has to recalculate the whole uh network topology over and over again. \
  \
 


### 00:08:19

  \
**Damir Simpovic:** So yeah that's that's the reason why um what uh we found to be working is the basically the headscale server playing um kubernetes without um any um without any uh cube spray of course cube spray will work however in that case uh the uh anible host will need to have access to the nodes which which is often not practical. So definitely plain K3s or K3S is recommended. Calico SD CNI uh with uh IBGP enabled and uh of course headscale which is the base for all this uh how tail scale works um uses wire guard uh to establish pointto-oint VPN tunnels um it's super efficient uh most uh new chips processors have uh wire guard they have um the crypto that wire guard uses which is the uh sorry which is the um I believe it's using uh a AES and with SHA 256 basically all modern processors have this in in chip on chip and it's built into Linux into the Linux kernel since I don't know which version um it uh traverses that over it's called DERP servers basically uh if it cannot establish a direct connection between then the host the nodes then the uh the uh connection will be established over a third party which is the derp server which is the headscale server itself. \
  \
 


### 00:10:08

  \
**Damir Simpovic:** Um it will of course prioritize local connections. So basically uh if there's more than one connection to uh another node or to a control plane then uh to use the most efficient one again it can use IPv6 uh and uh yeah it's using UDP as the protocol for uh which is basically what the wire guard uses. Um so yeah uh what is headscale? It's a free version of tail scale. Delta is paid. So anything over 100 nodes you have to pay for it. Uh you you also have to pay for the traffic. So yeah it comes with the cost. Uh headscale is self-hosted. So basically yourself uh and um below 100 or maybe 150 nodes it can be a super simple server. It doesn't need to be something very powerful. It can be a very cheap server somewhere in a data center. Um yeah so uh the testing part uh what we found is that the worker nodes do not seem to be overb burned by a thousand routes. \
  \
 


### 00:11:23

  \
**Damir Simpovic:** So what I'm talking about right here is uh we basically had a uh a and a cash provider with a thousand nodes actual thousand nodes and what happens is basically uh the BGP uh in the bird process inside Kico which is basically BGP is creating u uh pod networks uh which are /26 that means that it's like it's 64 uh it can host up to 64 pods and uh every node uh gets assigned one of these /26 um initially if if the pod count uh goes over 64 then it will receive another one and then another one etc. So basically what that means is that um every node in the network has to have a route uh to every other uh nodes pod network uh which uh yeah basically totals to over a thousand routes in the routing table of every node. Uh however uh testing has uh proved that uh this is actually not an issue. Uh so yeah uh the weakest link is the headscale server as I said uh it doesn't like changes in the network topology when new nodes are being added or uh removed uh or just disconnections and stuff like that then it's basically um recalculating the whole topology over and over again and uh if uh multiple um basically or 250 nodes is something that uh it can handle pretty much uh comfortably. \
  \
 


### 00:13:53

  \
**Damir Simpovic:** Um uh it's extremely CPU intensive this part uh the recalculation the uh topology recalculation um so yeah with with newer CPUs maybe um some CPUs that are more uh like high it had they have higher clock speeds over core core count which is not a very common thing in the data set that were but anyway those processors will uh will do better. Um yeah um at a thousand at over a thousand nodes it's basically falling apart. Uh the new routes cannot be added. Uh this the headscale service basically I managed to to onboard the 150 nodes but that was suffering basically. Um what we tested that didn't work was VXLAN which is the what's Akash providers are using right now. So VX line is basically uh a local area network on top of another local area network. Uh it can also be routed uh as an underlying network. However, in this case uh it would produce uh broadcast storms. So basically whenever a new node would be added it would basically uh kill the whole network. \
  \
 


### 00:15:21

  \
**Damir Simpovic:** uh we tested IP and IP in IP encapsulation also which is also a feature of calico it also produces similar results and it's also not very um it's not uh suggested to use IP in IP over uh delcale uh what didn't work also is was IBGP without headscale routed adver advertisements so basically not Not only does uh the IBGP internal process of the the the cluster has to know about every other network, uh the tail scale uh also the tail scale uh uh server also has to know about all the other networks both the uh pod networks and the obviously node networks. uh simply because uh those uh packets that are leaving if basically any node uh will hit the tail scale network and if it doesn't know where this particular uh network is uh uh on which node this particular network is hosted uh it will just discard the packet and uh nothing will work. But it also started um producing not in this case not broadcast on but routing loops. Uh again it would just kill the network. So yeah what uh did work was as I said before the headscale server plane uh Kubernetes uh we have a dedicated cluster which is on the same network uh as the control plane nodes. \
  \
 


### 00:17:16

  \
**Damir Simpovic:** So basically the control plane nodes have and the CD cluster uh we had to go with this uh topology because um otherwise um by default uh every Kubernetes cluster is uh new Kubernetes cluster um is either having CD configured as a system service that's with cube spray hosts if If you if you go with uh with plain uh uh Kubernetes then it will be set up as a pod uh or in in in case of K3S it will be baked in into the um K3S service itself. So basically inside the gator is binary and none of these is basically ideal because um the nodes have to be um advertised uh there the node IP like internal IP addresses have to be advertised using their tail scale addresses and if tail scale for any reason uh crashes or is unreachable for any reason uh it will basically uh render CD be uh uh un unusable and that basically leads to a lot of problems. So best is to use a dedicated HCD just three nodes that run only CD and then uh basically run uh Kubernetes on top of that. \
  \
 


### 00:18:52

  \
**Damir Simpovic:** Um uh Calico, yeah, that's standard with cash providers right now. However, with IBGP instead of uh VXLAN uh headscale route advertisement. So basically uh again every time a new node is added to the cluster uh and the first pod is spun up the uh control plane nodes and the API server will basically assign a a /26 uh um network to uh to that node but not before and uh yeah uh What that means is that uh basically we have to wait uh for uh the uh node to come up and then for this uh uh whole uh subnet or network to be assigned and then we have to advertise this uh network to the headscale server and then it will basically start routing and yeah as I said it's it was tested up to,50 nodes not not working very Well, the headscale server is suffering basically. Uh the best is uh between 100 and 250 nodes. That's where uh basically everything was working nice. Um the bandwidth considerations um uh it's it's counterintuitive. However, the headscale server u I I I wrote here that um 10g public network access is required or suggested but it's not really an a requirement. \
  \
 


### 00:20:36

  \
**Damir Simpovic:** Um uh why because uh headscale and test scale tail scale will try to establish direct links whenever possible before they try before they actually go over the drip server. So most of the time the headscale server from the traffic uh point of view will basically be unused. It will only be used for route advertisements and yeah don't uh uh uh likeliness probes and stuff like that which are not very bandwidth intensive. Um yeah I've already spoke about all of these before and yeah there you go. Any questions? \
**Zeke Ezagui:** Thanks, Damir. Um, so at this point, if there's no other question for provider console, Scott, can you give a brief update on um providers uh provider release coming up? \
**Scott Carruthers:** Yes, we have a a provider release that I currently um that we currently have. Zeke actually built the the providers in test net cluster that we haven't released to the public yet. Um we're currently experiencing a bug in that version um that the communications with associated RPC node becomes interrupted after about an hour or two of operation. \
  \
 


### 00:22:08

  \
**Scott Carruthers:** if you bounce the provider pod, it will resolve, but obviously that's a bug that we need to to cure um before releasing. So, uh I honestly don't have a release schedule. We should uh before the uh next mainet upgrade um which we're currently targeting for the end of September, we'll be uh releasing a new provider version with JWT and some other uh added features. Um, but like I said, currently we're uh re viewing a breaking bug. Um, so yeah, that's that's kind of the update. I would just ask the provider community to be um an alert in Discord messaging. Um, we'll definitely be releasing a we'll definitely have a new uh provider release coming out soon, but I don't know the exact uh time and date yet. \
**Zeke Ezagui:** Perfect. Any questions in terms of just general provider software releases? Cool. Thank you, Scott. Um at this point we'll open it up the floor to any questions with regards to anything providers on aos. Okay. Um if there's no more questions we can call this meeting ajourned. U thanks everybody for joining. Thank you Demir Dval Jagar and Scott for presenting and providing updates. Appreciate it. Um, and with that, we'll see you in the metaverse. Thanks everybody. \
  \
 


### Transcription ended after 00:23:58

